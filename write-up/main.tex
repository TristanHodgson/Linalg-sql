\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{style}

\title{Performance Regimes of In-Database Matrix Multiplication: A Communication-Aware Analysis}
\author{Tristan Hodgson}
\date{January 8, 2026}

\begin{document}
\maketitle

\begin{abstract}
Linear algebra operations are increasingly executed under non-classical computational constraints, including limited memory, high data-movement costs, and data that is natively stored in database systems. While specialised numerical libraries such as NumPy are optimised for dense, in-memory computation, relational databases offer alternative execution models that emphasise data locality and filtering. In this work, we empirically study the performance regimes of matrix multiplication when implemented directly within a relational database compared to a conventional Python-NumPy workflow. Focusing on sparse matrices stored in coordinate form, we analyse how sparsity, matrix dimension, and communication costs affect runtime. We develop a simple cost model that separates arithmetic complexity from data-movement overhead and show that, under sufficiently sparse regimes or constrained bandwidth, in-database computation can become competitive with or outperform traditional approaches, despite higher per-operation costs. Our results highlight the importance of communication-aware cost models when evaluating numerical algorithms in data-centric environments and illustrate how classical asymptotic analyses may fail to predict practical performance under realistic system constraints.
\end{abstract}

\section{Introduction}

Classical numerical linear algebra typically assumes that matrices are either dense or follow structured sparsity patterns, and that computation occurs entirely in-memory where floating-point operations (FLOPs) are the dominant cost. However, modern data-centric workflows increasingly violate these assumptions. In many applications, such as large-scale graph analytics and distributed machine learning, data is natively stored in relational databases. Furthermore, as enterprises shift toward "data-gravity" architectures to control the rising costs of data movement and cloud infrastructure \cite{dbta2026}, the traditional model of exporting data to specialized libraries like NumPy becomes a bottleneck.

Modern workflows are often characterized by:
\begin{itemize}
    \item Data stored in relational databases rather than flat binary files.
    \item High-dimensional matrices with extreme sparsity.
    \item Communication and I/O costs that dominate arithmetic processing.
\end{itemize}

The central question of this study is: \textit{When does changing the computational model change which algorithmic approach is preferable?} We do not propose new algorithms. Instead, we empirically study how standard formulations of matrix multiplication behave under differing computational and data-movement constraints.

\section{Related Work}

\subsection{Classical Matrix Multiplication}
The "gold standard" for dense matrix computations remains the work of Golub and Van Loan \cite{golub2013}, which defines the classical assumptions of $O(n^3)$ complexity for standard multiplication. Demmel \cite{demmel1997} further explores the impact of memory hierarchies and conditioning on algorithm stability, providing the framing for why arithmetic alone does not dictate performance in modern systems.

\subsection{Sparse Linear Algebra}
Efficient sparse computation relies heavily on data representation. While CSR and CSC formats are standard in numerical libraries, the coordinate (triplet) format is often natively supported by database structures \cite{davis2006}. As noted by Buluc and Gilbert \cite{buluc2011}, pruning zeros allows for effective complexity reductions that change based on sparsity patterns.

\subsection{Communication-Avoiding Computation}
The foundational work by Hong and Kung \cite{hong1981} established that data movement is a primary constraint in algorithm performance. This has evolved into modern "communication-avoiding" linear algebra \cite{ballard2014}, which seeks to minimize I/O overhead—a philosophy that aligns with in-database analytics where filtering occurs close to the storage layer \cite{stonebraker2018}.

\section{Computational Model and Problem Setup}

\subsection{Matrix Representation}
We represent matrices in coordinate format: $(i, j, value)$. This "triplet" form is native to relational tables. Any entry not explicitly stored is an implied zero, allowing for significant storage savings in sparse regimes.

\subsection{Computational Workflows}
We compare two distinct workflows:
\begin{itemize}
    \item \textbf{Python Workflow}: Requires transferring all non-zero entries from the database to a client, performing dense multiplication using NumPy, and uploading the result back to the server.
    \item \textbf{SQL Workflow}: Executes multiplication entirely within the database engine using joins and aggregations, eliminating client-side data transfer.
\end{itemize}

\subsection{Cost Decomposition}
Our analysis separates the total runtime into arithmetic cost, data-movement (communication) cost, and filtering effects. This allows us to move beyond descriptive benchmarks toward an analytical model of performance regimes.

\section{Cost Model}

\subsection{Python-NumPy Model}
NumPy leverages highly optimized BLAS/LAPACK routines. We model its runtime as:
$$t_{\text{Py}} \approx \alpha n^3 + \beta n^2$$
where $\alpha n^3$ represents the arithmetic work and $\beta n^2$ captures the $O(n^2)$ communication overhead required for data transfer.

\subsection{SQL-Based Model}
SQL exploits sparsity through the JOIN operator. For matrices with density $d$, the effective work scales with the number of non-zero intersections:
$$t_{\text{SQL}} \approx \gamma (d^2 n^3)$$
This model assumes independence in the distribution of non-zero entries.

\subsection{Regime Prediction}
The transition point where SQL becomes faster than NumPy ($t_{\text{SQL}} < t_{\text{Py}}$) occurs at a critical density $d_{\text{crit}}$:
$$d_{\text{crit}} = \sqrt{\frac{\alpha + \frac{\beta}{n}}{\gamma}}$$
We emphasize that this model predicts qualitative regime behavior rather than exact constants.

\section{Experimental Methodology}

\subsection{Matrix Generation}
Matrices were generated by sampling i.i.d. entries from a uniform distribution on $[0,1)$, masked by a Bernoulli parameter $d$ to control sparsity. We utilized a truncation method—generating the largest matrix first and slicing for smaller dimensions—to ensure consistency across tests.

\subsection{Benchmarking Setup}
Tests were conducted using PostgreSQL for SQL execution and NumPy for the Python workflow. We simulated a constrained network environment (10 Mbps) to observe the impact of bandwidth on $d_{\text{crit}}$.

\subsection{Limitations of the Experimental Setup}
Our setup is limited to synthetic random matrices; it does not account for hardware-specific optimizations (like GPUs) or specialized structured sparsity often found in real-world graphs.

\section{Results}

\subsection{Dense Regimes}
As expected, NumPy dominates in dense regimes due to highly optimized arithmetic kernels. Figures \ref{fig:alpha} and \ref{fig:benchmark} demonstrate that at high densities, the performance gap between NumPy and SQL grows with matrix size $n$.

\subsection{Sparse Regimes}
In sparse regimes, we observe a performance inversion. The ability of the SQL JOIN to filter out zeros before multiplication compensates for its higher per-operation cost.

\subsection{Transition Regions}
The $d_{\text{crit}}$ boundary (Figure \ref{fig:crit}) identifies the "competition zone." Within this region, performance is highly sensitive to parameters such as available network bandwidth and memory constraints.

\section{Discussion}

Our results demonstrate why asymptotic complexity alone is a misleading metric for algorithm selection in data-centric environments. While NumPy's $O(n^3)$ arithmetic is faster in a vacuum, the $O(n^2)$ communication cost of moving data becomes a bottleneck in constrained environments. This is particularly relevant for large-scale analytics and graph algorithms where data is too large to move efficiently between systems.

\section{Limitations and Future Work}

This study focuses purely on performance regimes and does not address:
\begin{itemize}
    \item Numerical stability or floating-point error accumulation.
    \item Structured sparsity patterns (e.g., power-law graphs).
    \item Extension to iterative methods like PageRank or PCA.
\end{itemize}
These areas represent critical avenues for strengthening the analysis of data-centric numerical methods.

\section{Conclusion}

This work demonstrates that the optimal computational model for matrix multiplication is not static but depends on the interplay of sparsity, dimension, and communication costs. Performance comparisons between numerical methods must be understood in terms of computational regimes rather than isolated benchmarks.

\bibliographystyle{plain}
\bibliography{references}

\end{document}